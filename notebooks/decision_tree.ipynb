{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization of Decision Tree Classifier Using Hill Climbing and Genetic Algorithms\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we focus on hyperparameter optimization for a **Decision Tree Classifier** using **Hill Climbing** and **Genetic Algorithms**. Decision trees are simple yet powerful models capable of capturing complex decision boundaries. However, they are prone to overfitting, making hyperparameter tuning crucial for optimal performance.\n",
    "\n",
    "Our goal is to fine-tune the hyperparameters to maximize the classifier's accuracy on the **Breast Cancer Wisconsin** dataset. By applying advanced optimization techniques, we aim to systematically explore the hyperparameter space and enhance the model's predictive capabilities.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We utilize the **Breast Cancer Wisconsin dataset**, consisting of **569 samples** with **30 numerical features**. The dataset provides a binary classification task to distinguish between malignant and benign tumors based on various measurements.\n",
    "\n",
    "## Hyperparameter Space\n",
    "\n",
    "We define a diverse hyperparameter space for the Decision Tree classifier:\n",
    "\n",
    "- **`criterion`**: Function to measure the quality of a split (`\"gini\"` or `\"entropy\"`).\n",
    "- **`splitter`**: Strategy used to choose the split at each node (`\"best\"` or `\"random\"`).\n",
    "- **`max_depth`**: Maximum depth of the tree. Values range from `None` (unlimited depth) to integers from 1 to 20.\n",
    "- **`min_samples_split`**: Minimum number of samples required to split an internal node (integer between 2 and 20).\n",
    "- **`min_samples_leaf`**: Minimum number of samples required to be at a leaf node (integer between 1 and 20).\n",
    "- **`max_features`**: Number of features to consider when looking for the best split (`None`, `\"auto\"`, `\"sqrt\"`, `\"log2\"`).\n",
    "\n",
    "## Optimization Methods\n",
    "\n",
    "### Hill Climbing Optimizer\n",
    "\n",
    "- **Description**: Iteratively explores neighboring configurations from a starting point, seeking improvements.\n",
    "- **Strengths**: Efficient for local search and requires less computational resources.\n",
    "- **Challenges**: Susceptible to becoming trapped in local optima.\n",
    "\n",
    "### Genetic Algorithm Optimizer\n",
    "\n",
    "- **Description**: Utilizes mechanisms inspired by biological evolution, such as selection, crossover, and mutation, to evolve a population of solutions.\n",
    "- **Strengths**: Capable of exploring a broad search space and avoiding premature convergence.\n",
    "- **Challenges**: Requires careful parameter tuning (e.g., population size, mutation rate) and more computational resources.\n",
    "\n",
    "## Evaluation Function\n",
    "\n",
    "Our evaluation function:\n",
    "\n",
    "- Creates a Decision Tree classifier using hyperparameters passed via `**config`.\n",
    "- Implements **Stratified K-Fold Cross-Validation** to ensure reliable performance estimates.\n",
    "- Employs fidelity levels corresponding to cross-validation folds, enabling a trade-off between evaluation cost and fidelity.\n",
    "\n",
    "## Goals\n",
    "\n",
    "- **Optimize Hyperparameters**: Discover the hyperparameter set that maximizes validation accuracy.\n",
    "- **Evaluate Optimization Techniques**: Assess which optimization strategy is more effective for this problem.\n",
    "- **Efficient Budget Utilization**: Complete the optimization within a specified computational budget.\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- Enhanced performance of the Decision Tree classifier through optimized hyperparameters.\n",
    "- Understanding of which hyperparameters most significantly influence model performance.\n",
    "- Performance comparison between Hill Climbing and Genetic Algorithms in the context of hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# Importing necessary classes from your module\n",
    "from focus_opt.hp_space import HyperParameterSpace, CategoricalHyperParameter, OrdinalHyperParameter, ContinuousHyperParameter\n",
    "from focus_opt.optimizers import HillClimbingOptimizer, GeneticAlgorithmOptimizer\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the hyperparameter space for a Decision Tree Classifier\n",
    "hp_space = HyperParameterSpace(\"Decision Tree HP Space\")\n",
    "\n",
    "hp_space.add_hp(CategoricalHyperParameter(name=\"criterion\", values=[\"gini\", \"entropy\"]))\n",
    "hp_space.add_hp(CategoricalHyperParameter(name=\"splitter\", values=[\"best\", \"random\"]))\n",
    "hp_space.add_hp(OrdinalHyperParameter(name=\"max_depth\", values=[None] + list(range(1, 21))))\n",
    "hp_space.add_hp(ContinuousHyperParameter(name=\"min_samples_split\", min_value=2, max_value=20, is_int=True))\n",
    "hp_space.add_hp(ContinuousHyperParameter(name=\"min_samples_leaf\", min_value=1, max_value=20, is_int=True))\n",
    "hp_space.add_hp(ContinuousHyperParameter(name=\"max_features\", min_value=0.0, max_value=1.0, step_size=0.05))\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Refactored evaluation function for Decision Tree Classifier\n",
    "def dt_evaluation(config: Dict[str, Any], fidelity: int) -> float:\n",
    "    \"\"\"\n",
    "    Evaluation function for a Decision Tree Classifier with cross-validation.\n",
    "\n",
    "    Args:\n",
    "        config (Dict[str, Any]): Configuration of hyperparameters.\n",
    "        fidelity (int): Fidelity level (index of the CV fold).\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy for the specified CV fold.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Evaluating config: {config} at fidelity level: {fidelity}\")\n",
    "\n",
    "    # Initialize the Decision Tree Classifier with the config as **kwargs\n",
    "    # Set a fixed random_state for reproducibility\n",
    "    clf = DecisionTreeClassifier(random_state=42, **config)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Get the train and test indices for the specified fold\n",
    "    for fold_index, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        if fold_index + 1 == fidelity:\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "            logging.info(f\"Score for config {config} at fold {fidelity}: {score}\")\n",
    "            return score\n",
    "\n",
    "    raise ValueError(f\"Invalid fidelity level: {fidelity}\")\n",
    "\n",
    "# Instantiate the HillClimbingOptimizer\n",
    "hill_climbing_optimizer = HillClimbingOptimizer(\n",
    "    hp_space=hp_space,\n",
    "    evaluation_function=dt_evaluation,\n",
    "    max_fidelity=5,\n",
    "    maximize=True,\n",
    "    log_results=True,\n",
    "    warm_start=20,\n",
    "    random_restarts=5,\n",
    ")\n",
    "\n",
    "# Run the Hill Climbing optimization\n",
    "best_candidate_hill_climbing = hill_climbing_optimizer.optimize(budget=500)\n",
    "print(f\"Best candidate from Hill Climbing: {best_candidate_hill_climbing.config} with score: {best_candidate_hill_climbing.evaluation_score}\")\n",
    "\n",
    "# Instantiate the GeneticAlgorithmOptimizer\n",
    "ga_optimizer = GeneticAlgorithmOptimizer(\n",
    "    hp_space=hp_space,\n",
    "    evaluation_function=dt_evaluation,\n",
    "    max_fidelity=5,\n",
    "    maximize=True,\n",
    "    population_size=20,\n",
    "    crossover_rate=0.8,\n",
    "    mutation_rate=0.1,\n",
    "    elitism=1,\n",
    "    tournament_size=3,\n",
    "    min_population_size=5,\n",
    "    log_results=True\n",
    ")\n",
    "\n",
    "# Run the Genetic Algorithm optimization\n",
    "best_candidate_ga = ga_optimizer.optimize(budget=500)\n",
    "print(f\"Best candidate from Genetic Algorithm: {best_candidate_ga.config} with score: {best_candidate_ga.evaluation_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best candidate from Hill Climbing: {best_candidate_hill_climbing.config} with score: {best_candidate_hill_climbing.evaluation_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best candidate from Genetic Algorithm: {best_candidate_ga.config} with score: {best_candidate_ga.evaluation_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
